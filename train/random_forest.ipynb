{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea87b17a",
   "metadata": {},
   "source": [
    "# 데이터셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea783f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uijong/miniforge3/envs/classifier/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import keyword\n",
    "import builtins\n",
    "import tokenize\n",
    "from io import StringIO\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "model.eval()  # Disable dropoutl\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "class LoopVisitor(ast.NodeVisitor):\n",
    "    def __init__(self):\n",
    "        self.max_depth = 0\n",
    "        self.current_depth = 0\n",
    "\n",
    "    def _visit_loop(self, node):\n",
    "        self.current_depth += 1\n",
    "        self.max_depth = max(self.max_depth, self.current_depth)\n",
    "        self.generic_visit(node)  # Visit children within the loop\n",
    "        self.current_depth -= 1\n",
    "\n",
    "    def visit_For(self, node):\n",
    "        self._visit_loop(node)\n",
    "\n",
    "    def visit_While(self, node):\n",
    "        self._visit_loop(node)\n",
    "\n",
    "    def visit_AsyncFor(self, node):\n",
    "        self._visit_loop(node)\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.features = [\n",
    "            \"avg_identifier_length\",\n",
    "            \"average_function_length\",\n",
    "            \"token_count\",\n",
    "            \"function_count\",\n",
    "            \"blank_ratio\",\n",
    "            \"identifier_count\",\n",
    "            \"total_lines\",\n",
    "            \"codebert_embedding\",\n",
    "            \"comment_ratio\",\n",
    "            \"max_control_depth\",\n",
    "        ]\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "        self.model = RobertaModel.from_pretrained(\"microsoft/codebert-base\").to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_avg_identifier_length(self, target: str) -> float:\n",
    "        \"\"\"\n",
    "        평균 식별자 길이\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(target)\n",
    "            identifiers = set()\n",
    "\n",
    "            class IdentifierLengthVisitor(ast.NodeVisitor):\n",
    "                def visit_Name(self, node):\n",
    "                    identifiers.add(node.id)\n",
    "                    self.generic_visit(node)\n",
    "\n",
    "                def visit_FunctionDef(self, node):\n",
    "                    identifiers.add(node.name)\n",
    "                    self.generic_visit(node)\n",
    "\n",
    "                def visit_ClassDef(self, node):\n",
    "                    identifiers.add(node.name)\n",
    "                    self.generic_visit(node)\n",
    "\n",
    "            IdentifierLengthVisitor().visit(tree)\n",
    "\n",
    "            if not identifiers:\n",
    "                return 0.0\n",
    "\n",
    "            total_length = sum(len(name) for name in identifiers)\n",
    "            return total_length / len(identifiers)\n",
    "        except SyntaxError:\n",
    "            return 0.0\n",
    "\n",
    "    def get_average_function_length(self, target: str) -> float:\n",
    "        \"\"\"\n",
    "        평균 함수 길이\n",
    "        함수가 없으면 0.0 반환\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(target)\n",
    "            lengths = []\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.FunctionDef):\n",
    "                    # Calculate end line more robustly\n",
    "                    end_lineno = node.lineno\n",
    "                    for sub_node in ast.walk(node):\n",
    "                        if hasattr(sub_node, 'lineno'):\n",
    "                            end_lineno = max(end_lineno, sub_node.lineno)\n",
    "                    lengths.append(end_lineno - node.lineno + 1)\n",
    "            return sum(lengths) / len(lengths) if lengths else 0.0\n",
    "        except SyntaxError:\n",
    "            return 0.0\n",
    "\n",
    "    def get_token_count(self, target: str) -> int:\n",
    "        \"\"\"\n",
    "        코드 전체 토큰 수 반환하는 함수\n",
    "        주석, 공백, 줄바꿈 토큰 제외\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tokens = tokenize.generate_tokens(StringIO(target).readline)\n",
    "            # Define tokens to skip more comprehensively\n",
    "            skip_tokens = {\n",
    "                tokenize.COMMENT,\n",
    "                tokenize.NL,  # Non-logical newline (e.g. inside parentheses)\n",
    "                tokenize.NEWLINE,  # Logical newline\n",
    "                tokenize.INDENT,\n",
    "                tokenize.DEDENT,\n",
    "                tokenize.ENCODING,  # Usually at the start, e.g. '# -*- coding: utf-8 -*-'\n",
    "                tokenize.ENDMARKER  # Marks the end of the file\n",
    "            }\n",
    "            # Additionally, filter out whitespace tokens if any are generated (usually not explicitly)\n",
    "            # tokenize.generate_tokens already skips most physical whitespace between tokens\n",
    "\n",
    "            count = 0\n",
    "            for tok in tokens:\n",
    "                if tok.type not in skip_tokens:\n",
    "                    # Filter out tokens that are purely whitespace, though `generate_tokens` usually handles this.\n",
    "                    # For example, a `tokenize.SPACE` type does not exist; spaces separate other tokens.\n",
    "                    # An ` tokenize.ERRORTOKEN` might represent things like standalone backslashes or invalid indent.\n",
    "                    if tok.type == tokenize.ERRORTOKEN and tok.string.isspace():\n",
    "                        continue\n",
    "                    count += 1\n",
    "            return count\n",
    "        except (tokenize.TokenError, IndentationError):\n",
    "            return 0\n",
    "\n",
    "    def get_function_count(self, target: str) -> int:\n",
    "        \"\"\"\n",
    "        함수 정의 개수\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(target)\n",
    "            return sum(isinstance(n, ast.FunctionDef) for n in ast.walk(tree))\n",
    "        except SyntaxError:\n",
    "            return 0\n",
    "\n",
    "    def get_blank_ratio(self, target: str) -> float:\n",
    "        lines = target.splitlines()\n",
    "        return sum(not line.strip() for line in lines) / len(lines) if lines else 0.0\n",
    "\n",
    "    def get_identifier_count(self, target: str) -> int:\n",
    "        \"\"\"\n",
    "        고유 식별자 개수 반환하는 함수\n",
    "        - 파이썬 키워드, 내장 함수는 제외\n",
    "        - 변수, 함수명, 매개변수, 속성 이름 등 포함\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(target)\n",
    "            names = set()\n",
    "            kw = set(keyword.kwlist)\n",
    "            built_in = set(dir(builtins))  # Using set(dir(builtins)) is fine for common builtins\n",
    "\n",
    "            class IdentifierVisitor(ast.NodeVisitor):\n",
    "                def visit_Name(self, node):\n",
    "                    \"\"\"\n",
    "                    식별자 수집\n",
    "                    \"\"\"\n",
    "                    if node.id not in kw and node.id not in built_in:\n",
    "                        names.add(node.id)\n",
    "                    self.generic_visit(node)\n",
    "\n",
    "                def visit_FunctionDef(self, node):\n",
    "                    # 함수 정의 식별자 수집\n",
    "                    if node.name not in kw and node.name not in built_in:\n",
    "                        names.add(node.name)\n",
    "                    for arg_node in node.args.args:\n",
    "                        if arg_node.arg not in kw and arg_node.arg not in built_in:\n",
    "                            names.add(arg_node.arg)\n",
    "                    if node.args.vararg and node.args.vararg.arg not in kw and node.args.vararg.arg not in built_in:\n",
    "                        names.add(node.args.vararg.arg)\n",
    "                    if node.args.kwarg and node.args.kwarg.arg not in kw and node.args.kwarg.arg not in built_in:\n",
    "                        names.add(node.args.kwarg.arg)\n",
    "                    for arg_node in node.args.kwonlyargs:\n",
    "                        if arg_node.arg not in kw and arg_node.arg not in built_in:\n",
    "                            names.add(arg_node.arg)\n",
    "                    self.generic_visit(node)\n",
    "\n",
    "                def visit_AsyncFunctionDef(self, node):\n",
    "                    # Async 함수 정의 식별자 수집\n",
    "                    self.visit_FunctionDef(node)\n",
    "\n",
    "                def visit_ClassDef(self, node):\n",
    "                    # 클래스 정의 식별자 수집\n",
    "                    if node.name not in kw and node.name not in built_in:\n",
    "                        names.add(node.name)\n",
    "                    self.generic_visit(node)\n",
    "\n",
    "                def visit_arg(self, node):\n",
    "                    # 매개변수 식별자 수집\n",
    "                    if node.arg not in kw and node.arg not in built_in:\n",
    "                        names.add(node.arg)\n",
    "                    self.generic_visit(node)\n",
    "\n",
    "                def visit_Attribute(self, node):\n",
    "                    # 속성 식별자 수집\n",
    "                    if node.attr not in kw and node.attr not in built_in:\n",
    "                        names.add(node.attr)\n",
    "                    self.visit(node.value)\n",
    "\n",
    "            IdentifierVisitor().visit(tree)\n",
    "            return len(names)\n",
    "        except SyntaxError:\n",
    "            return 0\n",
    "\n",
    "    def get_total_lines(self, target: str) -> int:\n",
    "        return len(target.splitlines())\n",
    "\n",
    "    def get_codebert_embedding(self, code: str) -> list:\n",
    "        inputs = self.tokenizer(code, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        codebert_embedding = outputs.last_hidden_state[0, 0]\n",
    "        return codebert_embedding.cpu().numpy().tolist()\n",
    "\n",
    "    def get_comment_ratio(self, target: str) -> float:\n",
    "        \"\"\"\n",
    "        주석 비율 계산\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tokens = tokenize.generate_tokens(StringIO(target).readline)\n",
    "            comment_count = sum(1 for tok in tokens if tok.type == tokenize.COMMENT)\n",
    "            total_count = sum(1 for tok in tokens if tok.type not in {tokenize.NL, tokenize.NEWLINE, tokenize.INDENT, tokenize.DEDENT, tokenize.ENCODING, tokenize.ENDMARKER})\n",
    "            return comment_count / total_count if total_count > 0 else 0.0\n",
    "        except (tokenize.TokenError, IndentationError):\n",
    "            return 0.0\n",
    "\n",
    "    def get_max_control_depth(self, target: str) -> int:\n",
    "        \"\"\"\n",
    "        최대 제어 구조 깊이 계산\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(target)\n",
    "            visitor = LoopVisitor()\n",
    "            visitor.visit(tree)\n",
    "            return visitor.max_depth\n",
    "        except SyntaxError:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66fb3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset written → /workspace/dataset/csv/final/python_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import csv\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = Path(\"dataset\")\n",
    "AI_CODES_DIR = BASE_DIR / \"ai_codes\"\n",
    "HUMAN_CODES_DIR = BASE_DIR / \"python_human_codes\"\n",
    "HUMAN_META_CSV = BASE_DIR / \"csv/human_metadata.csv\"\n",
    "OUTPUT_CSV = BASE_DIR / \"csv/python_dataset.csv\"\n",
    "\n",
    "EXTRACTOR = FeatureExtractor()\n",
    "HEADER: List[str] = (\n",
    "    [\"problem_id\", \"language\", \"code_size\", \"label\", \"model\"]\n",
    "    + EXTRACTOR.features\n",
    ")\n",
    "\n",
    "ENCODINGS = (\"utf-8\", \"latin1\", \"cp949\")\n",
    "\n",
    "def read_code(path: Path) -> str:\n",
    "    \"\"\"\n",
    "    주어진 경로에서 코드를 읽어오는 함수 (여러 인코딩으로 읽기 시도)\n",
    "    \"\"\"\n",
    "    for enc in ENCODINGS:\n",
    "        try:\n",
    "            return path.read_text(encoding=enc)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise UnicodeDecodeError(f\"Cannot decode {path}\")\n",
    "\n",
    "\n",
    "def iter_ai_files() -> Iterable[Path]:\n",
    "    \"\"\"\n",
    "    p****/ 디렉토리에서 모든 Python 파일을 읽어서 경로를 생성하는 제너레이터 함수\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pattern = str(AI_CODES_DIR / \"p*\" / \"*.py\")\n",
    "    yield from (Path(p) for p in glob.glob(pattern))\n",
    "\n",
    "\n",
    "def parse_ai_filename(path: Path) -> tuple[str, str]:\n",
    "    name_parts = path.stem.split(\"_\", 1)\n",
    "    problem_id = name_parts[0]\n",
    "    model = name_parts[1] if len(name_parts) == 2 else \"unknown\"\n",
    "    return problem_id, model\n",
    "\n",
    "\n",
    "def row_from_code(\n",
    "    *, problem_id: str | None, language: str, code_size: int, label: int,\n",
    "    model: str, code: str\n",
    ") -> List:\n",
    "    \"\"\"Assemble CSV row with extracted features.\"\"\"\n",
    "    features = [getattr(EXTRACTOR, f\"get_{f}\")(code) for f in EXTRACTOR.features]\n",
    "    return [problem_id, language, code_size, label, model] + features\n",
    "\n",
    "\n",
    "def generate_ai_rows() -> Iterable[List]:\n",
    "    \"\"\"\n",
    "    ai_codes 디렉토리에서 모든 Python 파일을 읽어서 CSV 행을 생성하는 함수\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for path in iter_ai_files():\n",
    "        problem_id, model = parse_ai_filename(path)\n",
    "        code = read_code(path)\n",
    "        yield row_from_code(\n",
    "            problem_id=problem_id,\n",
    "            language=\"Python\",\n",
    "            code_size=path.stat().st_size,\n",
    "            label=1,\n",
    "            model=model,\n",
    "            code=code,\n",
    "        )\n",
    "\n",
    "\n",
    "def generate_human_rows() -> Iterable[List]:\n",
    "    \"\"\"\n",
    "    python_human_codes 디렉토리에서 모든 Python 파일을 읽어서 CSV 행을 생성하는 함수\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    meta_df = pd.read_csv(HUMAN_META_CSV, dtype=str)\n",
    "\n",
    "    for _, row in meta_df.iterrows():\n",
    "        if row.get(\"language\") == \"Python\":\n",
    "            code_path = HUMAN_CODES_DIR / f\"{row['submission_id']}.py\"\n",
    "            if not code_path.exists():\n",
    "                print(f\"[WARN] 코드 파일 없음 → {code_path}\")\n",
    "                continue\n",
    "\n",
    "            code = read_code(code_path)\n",
    "            yield row_from_code(\n",
    "                problem_id=row.get(\"problem_id\"),\n",
    "                language=row.get(\"language\", \"Python\"),\n",
    "                code_size=code_path.stat().st_size,\n",
    "                label=int(row.get(\"label\", 0)),\n",
    "                model=row.get(\"Model\", \"human\"),\n",
    "                code=code,\n",
    "            )\n",
    "\n",
    "def main() -> None:\n",
    "    OUTPUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with OUTPUT_CSV.open(\"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(HEADER)\n",
    "\n",
    "        for row in generate_ai_rows():\n",
    "            writer.writerow(row)\n",
    "\n",
    "        for row in generate_human_rows():\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"[INFO] Dataset written → {OUTPUT_CSV.resolve()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8fd6d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "# CSV 로딩\n",
    "df = pd.read_csv('python_dataset.csv')\n",
    "\n",
    "# 문자열로 저장된 리스트를 파싱 (ast.literal_eval 사용)\n",
    "df['codebert_embedding'] = df['codebert_embedding'].apply(ast.literal_eval)\n",
    "\n",
    "# 768차원 벡터로 확장\n",
    "codebert_df = pd.DataFrame(df['codebert_embedding'].tolist(), index=df.index)\n",
    "codebert_df.columns = [f'codebert_{i}' for i in range(768)]\n",
    "\n",
    "# 기존 df에서 codebert_embedding 컬럼 제거 후 결합\n",
    "df = df.drop(columns=['codebert_embedding'])\n",
    "df = pd.concat([df, codebert_df], axis=1)\n",
    "df.to_csv('dataset/csv/final/python_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51951abb",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "932f4259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    RepeatedStratifiedKFold,\n",
    "    RandomizedSearchCV,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                             accuracy_score, balanced_accuracy_score,\n",
    "                             f1_score, roc_auc_score, log_loss,\n",
    "                             top_k_accuracy_score)\n",
    "\n",
    "# 샘플링 범위(무작위 탐색용)\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import make_scorer, top_k_accuracy_score\n",
    "\n",
    "\n",
    "df = pd.read_csv('../dataset/csv/python_dataset.csv')\n",
    "# df = pd.read_csv('../dataset/csv/cpp_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25a92873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Feature 및 데이터 로딩\n",
    "features = [\n",
    "    \"avg_identifier_length\",\n",
    "    \"average_function_length\",\n",
    "    \"token_count\",\n",
    "    \"function_count\",\n",
    "    \"blank_ratio\",\n",
    "    \"identifier_count\",\n",
    "    \"total_lines\",\n",
    "    \"code_size\",\n",
    "    \"max_control_depth\",\n",
    "    \"comment_ratio\",\n",
    "] + [f'codebert_{i}' for i in range(768)]\n",
    "\n",
    "# Cpp Feature\n",
    "# features = [\n",
    "#     \"code_size\",\n",
    "#     \"total_lines\",\n",
    "#     \"blank_ratio\",\n",
    "#     \"comment_ratio\",\n",
    "#     \"num_funcs\",\n",
    "#     \"avg_func_length\",\n",
    "#     \"max_control_depth\",\n",
    "#     \"control_count\",\n",
    "#     \"unique_identifiers\",\n",
    "#     \"token_count\"\n",
    "# ] + [f'vec_{i}' for i in range(768)]\n",
    "\n",
    "X = df[features]\n",
    "# 이진 분류면 'label', 다중 분류면 'model'·'language' 등으로 교체\n",
    "y = df[\"model\"]\n",
    "\n",
    "le = LabelEncoder()               # 문자열 → 정수 라벨\n",
    "y  = le.fit_transform(df[\"model\"])\n",
    "\n",
    "# 학습/테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14d485b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline # Imbalanced dataset을 위해서 사용\n",
    "from imblearn.over_sampling import SMOTE  # SMOTE 오버샘플링 기법\n",
    "\n",
    "smote = SMOTE(sampling_strategy='not majority', # 모든 소수 클래스를 250개로\n",
    "              k_neighbors=5, random_state=42)\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"pca\", PCA(n_components=80, random_state=42)),\n",
    "        ('smote', smote),\n",
    "        (\n",
    "            \"rf\",\n",
    "            RandomForestClassifier(\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                class_weight=None, # SMOTE로 오버샘플링했으므로 None으로 설정, 아니라면 \"balanced\"로 설정\n",
    "                bootstrap=True,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24545cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    \"pca__n_components\": randint(30, 150), # PCA 하이퍼파라미터\n",
    "    \"smote__k_neighbors\": randint(3, 8), # KMeansSMOTE 이웃 개수\n",
    "    \"rf__n_estimators\": randint(150, 600),\n",
    "    \"rf__max_depth\": randint(5, 40),\n",
    "    \"rf__min_samples_split\": randint(2, 20),\n",
    "    \"rf__min_samples_leaf\": randint(1, 20),\n",
    "    \"rf__max_features\": [\"sqrt\", \"log2\", None],\n",
    "}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42) # Cross validation인데, 각 fold 마다 stratified(안겹치게) sampling을 적용\n",
    "\n",
    "labels = np.unique(y)\n",
    "top2_acc_scorer = make_scorer(\n",
    "    score_func=top_k_accuracy_score,\n",
    "    response_method=\"predict_proba\",\n",
    "    k=2,\n",
    "    labels=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4caf024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 100 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best Params: {'pca__n_components': 103, 'rf__max_depth': 30, 'rf__max_features': 'log2', 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 10, 'rf__n_estimators': 540, 'smote__k_neighbors': 6}\n",
      "✅ Best CV F1 Score: 0.907\n"
     ]
    }
   ],
   "source": [
    "# 학습 실행\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100, # 무작위 탐색 횟수\n",
    "    scoring={\"top2\": top2_acc_scorer},\n",
    "    refit=\"top2\", # top2 기준으로 최적 모델 선택\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "print(\"✅ Best Params:\", search.best_params_)\n",
    "print(\"✅ Best CV F1 Score:\", search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0784448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy : 0.784\n",
      "Balanced acc. : 0.6880000000000001\n",
      "Macro F1      : 0.7076065926455667\n",
      "Weighted F1   : 0.7727815916098543\n",
      "Top-2 accuracy: 0.932\n",
      "Log-loss      : 1.0519866249213532\n",
      "ROC-AUC macro : 0.9344717037037036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Human       0.82      0.93      0.87       250\n",
      "    deepseek       0.74      0.56      0.64        50\n",
      "      gemini       0.84      0.82      0.83        50\n",
      "         gpt       0.71      0.78      0.74        50\n",
      "       grok3       0.68      0.38      0.49        50\n",
      "     mistral       0.70      0.66      0.68        50\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.75      0.69      0.71       500\n",
      "weighted avg       0.78      0.78      0.77       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_clf = search.best_estimator_\n",
    "y_pred  = best_clf.predict(X_test)\n",
    "y_proba = best_clf.predict_proba(X_test)\n",
    "\n",
    "print(\"Top-1 accuracy :\", accuracy_score(y_test, y_pred))\n",
    "print(\"Balanced acc. :\", balanced_accuracy_score(y_test, y_pred))\n",
    "print(\"Macro F1      :\", f1_score(y_test, y_pred, average='macro'))\n",
    "print(\"Weighted F1   :\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Top-2 accuracy:\", top_k_accuracy_score(y_test, y_proba, k=2))\n",
    "print(\"Log-loss      :\", log_loss(y_test, y_proba))\n",
    "print(\"ROC-AUC macro :\", roc_auc_score(y_test, y_proba,\n",
    "                                       multi_class='ovr', average='macro'))\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee29a236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy : 0.792\n",
      "Balanced acc. : 0.7040000000000001\n",
      "Macro F1      : 0.7182152438326729\n",
      "Weighted F1   : 0.7796083915826225\n",
      "Top-2 accuracy: 0.93\n",
      "Log-loss      : 0.7418482655203131\n",
      "ROC-AUC macro : 0.9460810370370369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Human       0.82      0.92      0.87       250\n",
      "    deepseek       0.78      0.62      0.69        50\n",
      "      gemini       0.75      0.84      0.79        50\n",
      "         gpt       0.72      0.76      0.74        50\n",
      "       grok3       0.85      0.34      0.49        50\n",
      "     mistral       0.73      0.74      0.73        50\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.77      0.70      0.72       500\n",
      "weighted avg       0.79      0.79      0.78       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "best_rf = search.best_estimator_            # 랜덤 서치로 찾은 최적 RF\n",
    "calibrated = CalibratedClassifierCV(best_rf,\n",
    "                                    method=\"isotonic\",\n",
    "                                    cv=3,    # 3-fold 내부 교차검증\n",
    "                                    ensemble=True)  # 기본값\n",
    "calibrated.fit(X_train, y_train)\n",
    "\n",
    "# calibrated 가 최종 모델 역할\n",
    "y_pred  = calibrated.predict(X_test)\n",
    "y_proba = calibrated.predict_proba(X_test)\n",
    "\n",
    "print(\"Top-1 accuracy :\", accuracy_score(y_test, y_pred))\n",
    "print(\"Balanced acc. :\", balanced_accuracy_score(y_test, y_pred))\n",
    "print(\"Macro F1      :\", f1_score(y_test, y_pred, average='macro'))\n",
    "print(\"Weighted F1   :\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Top-2 accuracy:\", top_k_accuracy_score(y_test, y_proba, k=2,\n",
    "                                              labels=np.unique(y_test)))\n",
    "print(\"Log-loss      :\", log_loss(y_test, y_proba))\n",
    "print(\"ROC-AUC macro :\", roc_auc_score(y_test, y_proba,\n",
    "                                       multi_class='ovr', average='macro'))\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3a9c37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_python_multilabel_classifier.joblib']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최종 모델 저장\n",
    "import joblib, sklearn\n",
    "\n",
    "bundle = dict(\n",
    "    encoder = le,\n",
    "    model = calibrated,\n",
    "    features = features,\n",
    "    sklearn_version = sklearn.__version__\n",
    ")\n",
    "joblib.dump(bundle, \"rf_python_multilabel_classifier.joblib\", compress=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
